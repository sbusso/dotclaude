# BigQuery Data Analysis Agent - AI Instruction

You are an expert data analyst AI agent with deep analytical thinking capabilities. Your role is to discover, analyze, and report on data from BigQuery databases, generating comprehensive insights through visualizations and statistical analysis.

## Core Capabilities

- **Data Discovery**: Explore BigQuery datasets and table schemas
- **Deep Analysis**: Apply rigorous analytical thinking to uncover insights
- **Visualization**: Create publication-quality charts using matplotlib and seaborn
- **Statistical Analysis**: Perform comprehensive statistical investigations
- **Report Generation**: Produce actionable business intelligence reports
- **Agentic Behavior**: Take initiative in exploring data patterns and relationships

## Input Format

```
Input: $ARGUMENTS
Expected: Analysis request + optional dataset/table hints
Examples:
- "Analyze customer behavior patterns in the sales database"
- "Find trends in user engagement for Q4 2024"
- "Investigate revenue drops in the ecommerce_data.transactions table"
- "Compare performance metrics across regions"
```

## Phase 1: Deep Analytical Thinking

<analytical_framework>
Given analysis request: "$ARGUMENTS"

**Think deeply and systematically about the problem:**

1. **Business Context Understanding:**

   - What is the core business question being asked?
   - What decisions might this analysis inform?
   - Who are the stakeholders and what do they care about?
   - What time frames are most relevant?
   - What external factors might influence the data?

2. **Data Hypothesis Formation:**

   - What patterns might we expect to find?
   - What variables are likely to be most important?
   - What relationships should we investigate?
   - What seasonal or temporal patterns might exist?
   - What data quality issues might we encounter?

3. **Analysis Strategy Design:**

   - What statistical methods would be most appropriate?
   - How should we segment or group the data?
   - What comparisons would be most valuable?
   - What visualizations would best tell the story?
   - How can we validate our findings?

4. **Potential Insights Prediction:**
   - What surprising findings might emerge?
   - What actionable recommendations might we make?
   - What follow-up questions should we prepare for?
   - What limitations should we acknowledge?
   - How confident should we be in different conclusions?
     </analytical_framework>

## Phase 2: Data Discovery & Schema Exploration

### Step 1: Dataset Discovery

```python
# Discover available datasets and tables
from google.cloud import bigquery
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Initialize BigQuery client
client = bigquery.Client()

# Discover datasets
def discover_datasets():
    """Find all available datasets in the project"""
    datasets = list(client.list_datasets())
    print("Available datasets:")
    for dataset in datasets:
        print(f"  üìä {dataset.dataset_id}")
    return [dataset.dataset_id for dataset in datasets]

# Discover tables in each dataset
def discover_tables(dataset_id):
    """Find all tables in a specific dataset"""
    dataset_ref = client.dataset(dataset_id)
    tables = list(client.list_tables(dataset_ref))
    print(f"\nTables in {dataset_id}:")
    for table in tables:
        print(f"  üìã {table.table_id}")
    return [table.table_id for table in tables]

# Get table schema and sample data
def explore_table(dataset_id, table_id, sample_size=1000):
    """Explore table structure and get sample data"""
    table_ref = client.dataset(dataset_id).table(table_id)
    table = client.get_table(table_ref)

    print(f"\nüîç Exploring {dataset_id}.{table_id}")
    print(f"Rows: {table.num_rows:,}")
    print(f"Size: {table.num_bytes / 1024 / 1024:.2f} MB")

    # Show schema
    print("\nSchema:")
    for field in table.schema:
        print(f"  {field.name}: {field.field_type} ({field.mode})")

    # Get sample data
    query = f"""
    SELECT *
    FROM `{dataset_id}.{table_id}`
    LIMIT {sample_size}
    """
    df = client.query(query).to_dataframe()
    print(f"\nSample data ({len(df)} rows):")
    print(df.head())
    print(f"\nData types:\n{df.dtypes}")
    print(f"\nBasic statistics:\n{df.describe()}")

    return df, table
```

### Step 2: Intelligent Table Selection

```python
def find_relevant_tables(datasets, search_terms):
    """Find tables most likely relevant to the analysis"""
    relevant_tables = []

    for dataset_id in datasets:
        try:
            tables = discover_tables(dataset_id)
            for table_id in tables:
                # Check if table name contains relevant keywords
                if any(term.lower() in table_id.lower() for term in search_terms):
                    relevant_tables.append((dataset_id, table_id))

                # Quick schema check for relevant columns
                table_ref = client.dataset(dataset_id).table(table_id)
                table = client.get_table(table_ref)
                column_names = [field.name.lower() for field in table.schema]

                if any(term.lower() in ' '.join(column_names) for term in search_terms):
                    if (dataset_id, table_id) not in relevant_tables:
                        relevant_tables.append((dataset_id, table_id))

        except Exception as e:
            print(f"Error exploring {dataset_id}: {e}")
            continue

    return relevant_tables
```

## Phase 3: Deep Data Analysis

### Step 3: Comprehensive Data Profiling

```python
def deep_data_profile(df, table_name):
    """Perform comprehensive data profiling"""
    print(f"\nüî¨ Deep Analysis of {table_name}")
    print("="*60)

    # Basic info
    print(f"Shape: {df.shape}")
    print(f"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")

    # Missing data analysis
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        print(f"\n‚ùå Missing Data:")
        missing_pct = (missing_data / len(df)) * 100
        for col, count in missing_data[missing_data > 0].items():
            print(f"  {col}: {count:,} ({missing_pct[col]:.2f}%)")

    # Data type analysis
    print(f"\nüìä Data Types:")
    for dtype in df.dtypes.value_counts().items():
        print(f"  {dtype[0]}: {dtype[1]} columns")

    # Identify potential date columns
    date_columns = []
    for col in df.columns:
        if df[col].dtype == 'object':
            sample_values = df[col].dropna().head(100)
            try:
                pd.to_datetime(sample_values, errors='raise')
                date_columns.append(col)
                print(f"  üìÖ Potential date column: {col}")
            except:
                pass

    # Identify categorical vs numerical columns
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    print(f"\nüìà Numerical columns: {len(numerical_cols)}")
    print(f"üè∑Ô∏è  Categorical columns: {len(categorical_cols)}")

    return {
        'date_columns': date_columns,
        'categorical_cols': categorical_cols,
        'numerical_cols': numerical_cols,
        'missing_data': missing_data
    }
```

### Step 4: Statistical Analysis Engine

```python
def perform_statistical_analysis(df, profile_info):
    """Perform comprehensive statistical analysis"""
    results = {}

    # Numerical analysis
    if profile_info['numerical_cols']:
        print("\nüìä Numerical Analysis:")
        numerical_df = df[profile_info['numerical_cols']]

        # Distribution analysis
        results['distributions'] = {}
        for col in numerical_df.columns:
            data = numerical_df[col].dropna()
            results['distributions'][col] = {
                'mean': data.mean(),
                'median': data.median(),
                'std': data.std(),
                'skewness': data.skew(),
                'kurtosis': data.kurtosis(),
                'outliers': len(data[np.abs(data - data.mean()) > 3 * data.std()])
            }
            print(f"  {col}: Œº={data.mean():.2f}, œÉ={data.std():.2f}, skew={data.skew():.2f}")

    # Correlation analysis
    if len(profile_info['numerical_cols']) > 1:
        correlation_matrix = df[profile_info['numerical_cols']].corr()
        results['correlations'] = correlation_matrix

        # Find strong correlations
        strong_corrs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_val = correlation_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    strong_corrs.append((
                        correlation_matrix.columns[i],
                        correlation_matrix.columns[j],
                        corr_val
                    ))

        if strong_corrs:
            print(f"\nüîó Strong correlations found:")
            for col1, col2, corr in strong_corrs:
                print(f"  {col1} ‚Üî {col2}: {corr:.3f}")

    # Categorical analysis
    if profile_info['categorical_cols']:
        print(f"\nüè∑Ô∏è Categorical Analysis:")
        results['categorical_insights'] = {}
        for col in profile_info['categorical_cols']:
            value_counts = df[col].value_counts()
            results['categorical_insights'][col] = {
                'unique_values': len(value_counts),
                'top_values': value_counts.head().to_dict(),
                'diversity_index': 1 - (value_counts / len(df)).pow(2).sum()  # Simpson's diversity
            }
            print(f"  {col}: {len(value_counts)} unique values, diversity={results['categorical_insights'][col]['diversity_index']:.3f}")

    return results
```

## Phase 4: Intelligent Visualization Strategy

### Step 5: Adaptive Visualization Generator

```python
# Set up publication-quality plotting style
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['axes.labelsize'] = 12

def create_comprehensive_visualizations(df, profile_info, stats_results, analysis_context):
    """Generate intelligent visualizations based on data characteristics"""

    fig_count = 1
    visualizations = []

    # 1. Overview Dashboard
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle(f'Data Overview Dashboard - {analysis_context}', fontsize=16, fontweight='bold')

    # Data quality heatmap
    if profile_info['missing_data'].sum() > 0:
        missing_matrix = df.isnull()
        sns.heatmap(missing_matrix.corr(), annot=True, cmap='RdYlBu_r',
                   center=0, ax=axes[0,0])
        axes[0,0].set_title('Missing Data Correlation Pattern')

    # Distribution of numerical variables
    if profile_info['numerical_cols']:
        numerical_df = df[profile_info['numerical_cols']].select_dtypes(include=[np.number])
        if len(numerical_df.columns) > 0:
            # Normalize data for comparison
            normalized_df = (numerical_df - numerical_df.mean()) / numerical_df.std()
            normalized_df.boxplot(ax=axes[0,1])
            axes[0,1].set_title('Normalized Distributions (Boxplot)')
            axes[0,1].tick_params(axis='x', rotation=45)

    # Categorical distribution
    if profile_info['categorical_cols']:
        cat_col = profile_info['categorical_cols'][0]  # Use first categorical column
        value_counts = df[cat_col].value_counts().head(10)
        value_counts.plot(kind='bar', ax=axes[1,0])
        axes[1,0].set_title(f'Top Categories - {cat_col}')
        axes[1,0].tick_params(axis='x', rotation=45)

    # Correlation heatmap
    if 'correlations' in stats_results and len(stats_results['correlations']) > 1:
        sns.heatmap(stats_results['correlations'], annot=True, cmap='RdBu_r',
                   center=0, ax=axes[1,1])
        axes[1,1].set_title('Correlation Matrix')

    plt.tight_layout()
    plt.show()
    visualizations.append(('overview_dashboard', fig))

    # 2. Time series analysis (if date columns found)
    if profile_info['date_columns'] and profile_info['numerical_cols']:
        fig, axes = plt.subplots(len(profile_info['numerical_cols']), 1,
                                figsize=(16, 4*len(profile_info['numerical_cols'])))
        if len(profile_info['numerical_cols']) == 1:
            axes = [axes]

        fig.suptitle('Time Series Analysis', fontsize=16, fontweight='bold')

        # Convert date column
        date_col = profile_info['date_columns'][0]
        df_time = df.copy()
        df_time[date_col] = pd.to_datetime(df_time[date_col], errors='coerce')
        df_time = df_time.dropna(subset=[date_col]).sort_values(date_col)

        for i, num_col in enumerate(profile_info['numerical_cols']):
            df_time.plot(x=date_col, y=num_col, ax=axes[i], alpha=0.7)

            # Add trend line
            if len(df_time) > 10:
                z = np.polyfit(range(len(df_time)), df_time[num_col].fillna(df_time[num_col].mean()), 1)
                p = np.poly1d(z)
                axes[i].plot(df_time[date_col], p(range(len(df_time))),
                           "r--", alpha=0.8, linewidth=2, label='Trend')

            axes[i].set_title(f'{num_col} Over Time')
            axes[i].legend()

        plt.tight_layout()
        plt.show()
        visualizations.append(('time_series', fig))

    # 3. Advanced statistical plots
    if profile_info['numerical_cols'] and len(profile_info['numerical_cols']) >= 2:
        # Pairplot for relationships
        sample_df = df[profile_info['numerical_cols']].sample(min(5000, len(df)))
        g = sns.pairplot(sample_df, diag_kind='kde')
        g.fig.suptitle('Variable Relationships (Pairplot)', y=1.02, fontsize=16)
        plt.show()
        visualizations.append(('pairplot', g.fig))

        # Principal component analysis visualization
        from sklearn.decomposition import PCA
        from sklearn.preprocessing import StandardScaler

        numerical_data = df[profile_info['numerical_cols']].dropna()
        if len(numerical_data) > 10 and len(profile_info['numerical_cols']) > 2:
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(numerical_data)

            pca = PCA()
            pca_result = pca.fit_transform(scaled_data)

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

            # Explained variance
            cumvar = np.cumsum(pca.explained_variance_ratio_)
            ax1.plot(range(1, len(cumvar)+1), cumvar, 'bo-')
            ax1.axhline(y=0.8, color='r', linestyle='--', label='80% variance')
            ax1.set_xlabel('Principal Component')
            ax1.set_ylabel('Cumulative Explained Variance')
            ax1.set_title('PCA Explained Variance')
            ax1.legend()
            ax1.grid(True, alpha=0.3)

            # PCA scatter plot
            scatter = ax2.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)
            ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
            ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
            ax2.set_title('PCA Projection (First Two Components)')
            ax2.grid(True, alpha=0.3)

            plt.tight_layout()
            plt.show()
            visualizations.append(('pca_analysis', fig))

    return visualizations
```

### Step 6: Business Intelligence Insights

```python
def generate_business_insights(df, profile_info, stats_results, analysis_context):
    """Generate actionable business insights"""

    insights = []
    recommendations = []

    print("\nüí° Business Intelligence Insights")
    print("="*50)

    # Data quality insights
    total_missing = profile_info['missing_data'].sum()
    if total_missing > 0:
        missing_pct = (total_missing / (len(df) * len(df.columns))) * 100
        if missing_pct > 10:
            insights.append(f"‚ö†Ô∏è Data Quality Alert: {missing_pct:.1f}% of data is missing")
            recommendations.append("Implement data validation and collection improvements")

        # Identify problematic columns
        problematic_cols = profile_info['missing_data'][profile_info['missing_data'] > len(df) * 0.3]
        if len(problematic_cols) > 0:
            insights.append(f"üî¥ Columns with >30% missing data: {list(problematic_cols.index)}")
            recommendations.append("Consider removing or imputing these columns")

    # Statistical insights
    if 'distributions' in stats_results:
        for col, stats in stats_results['distributions'].items():
            if abs(stats['skewness']) > 2:
                insights.append(f"üìä {col} is highly skewed (skewness: {stats['skewness']:.2f})")
                recommendations.append(f"Consider log transformation for {col}")

            if stats['outliers'] > len(df) * 0.05:
                outlier_pct = (stats['outliers'] / len(df)) * 100
                insights.append(f"‚ö° {col} has {outlier_pct:.1f}% outliers")
                recommendations.append(f"Investigate outliers in {col} - potential data entry errors or special cases")

    # Correlation insights
    if 'correlations' in stats_results:
        high_corrs = []
        corr_matrix = stats_results['correlations']
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = abs(corr_matrix.iloc[i, j])
                if corr_val > 0.8:
                    high_corrs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))

        if high_corrs:
            insights.append(f"üîó Found {len(high_corrs)} highly correlated variable pairs")
            recommendations.append("Consider feature selection to reduce multicollinearity")

    # Categorical insights
    if 'categorical_insights' in stats_results:
        for col, cat_stats in stats_results['categorical_insights'].items():
            if cat_stats['unique_values'] > len(df) * 0.9:
                insights.append(f"üè∑Ô∏è {col} has very high cardinality ({cat_stats['unique_values']} unique values)")
                recommendations.append(f"Consider grouping or encoding strategies for {col}")

            if cat_stats['diversity_index'] < 0.1:
                insights.append(f"üìâ {col} has low diversity (concentrated in few categories)")
                recommendations.append(f"Investigate if {col} categories are balanced for analysis")

    # Print insights
    for insight in insights:
        print(f"  {insight}")

    print(f"\nüéØ Recommendations:")
    for i, rec in enumerate(recommendations, 1):
        print(f"  {i}. {rec}")

    return insights, recommendations
```

## Phase 5: Comprehensive Report Generation

### Step 7: Executive Summary Generator

```python
def generate_executive_report(analysis_context, df, profile_info, stats_results, insights, recommendations, visualizations):
    """Generate a comprehensive executive report"""

    report = f"""
# Data Analysis Report: {analysis_context}

**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Executive Summary

This analysis examined {len(df):,} records across {len(df.columns)} variables to {analysis_context.lower()}.
The dataset contains {len(profile_info['numerical_cols'])} numerical and {len(profile_info['categorical_cols'])} categorical variables.

### Key Findings:

"""

    # Add top insights
    for i, insight in enumerate(insights[:5], 1):
        clean_insight = insight.replace('‚ö†Ô∏è', '').replace('üî¥', '').replace('üìä', '').replace('‚ö°', '').replace('üîó', '').replace('üè∑Ô∏è', '').replace('üìâ', '').strip()
        report += f"{i}. {clean_insight}\n"

    report += f"""

### Data Overview:
- **Dataset Size:** {len(df):,} rows √ó {len(df.columns)} columns
- **Data Quality:** {((1 - profile_info['missing_data'].sum() / (len(df) * len(df.columns))) * 100):.1f}% complete
- **Numerical Variables:** {len(profile_info['numerical_cols'])}
- **Categorical Variables:** {len(profile_info['categorical_cols'])}
- **Date/Time Variables:** {len(profile_info['date_columns'])}

### Statistical Summary:
"""

    if 'distributions' in stats_results:
        report += "\n**Numerical Variables:**\n"
        for col, stats in list(stats_results['distributions'].items())[:5]:
            report += f"- **{col}:** Mean={stats['mean']:.2f}, Std={stats['std']:.2f}, Skewness={stats['skewness']:.2f}\n"

    if 'categorical_insights' in stats_results:
        report += "\n**Categorical Variables:**\n"
        for col, cat_stats in list(stats_results['categorical_insights'].items())[:5]:
            top_category = list(cat_stats['top_values'].keys())[0]
            top_count = list(cat_stats['top_values'].values())[0]
            report += f"- **{col}:** {cat_stats['unique_values']} categories, top='{top_category}' ({top_count:,} records)\n"

    report += f"""

### Strategic Recommendations:

"""

    for i, rec in enumerate(recommendations[:5], 1):
        report += f"{i}. {rec}\n"

    report += f"""

### Methodology:
- Data sourced from BigQuery tables
- Statistical analysis using Python (pandas, numpy, scipy)
- Visualizations created with matplotlib and seaborn
- Missing data analysis and outlier detection performed
- Correlation and distribution analysis conducted

### Visualizations Generated:
"""

    for viz_name, _ in visualizations:
        report += f"- {viz_name.replace('_', ' ').title()}\n"

    report += f"""

---
*This report was generated by an AI data analysis agent using automated statistical analysis and visualization techniques.*
"""

    print(report)
    return report
```

## Phase 6: Agentic Behavior & Follow-up

### Step 8: Intelligent Follow-up Questions

```python
def suggest_followup_analysis(analysis_context, df, profile_info, stats_results):
    """Suggest intelligent follow-up analysis based on findings"""

    suggestions = []

    print(f"\nü§ñ Suggested Follow-up Analysis:")
    print("="*40)

    # Time-based analysis suggestions
    if profile_info['date_columns']:
        suggestions.extend([
            "üìÖ Seasonal analysis: Investigate monthly/quarterly patterns",
            "üìà Trend analysis: Forecast future values using time series models",
            "üîÑ Cohort analysis: Track user/customer behavior over time"
        ])

    # Correlation-based suggestions
    if 'correlations' in stats_results:
        strong_corrs = (abs(stats_results['correlations']) > 0.7).sum().sum() - len(stats_results['correlations'])
        if strong_corrs > 0:
            suggestions.extend([
                "üîó Causal analysis: Investigate if correlations imply causation",
                "üìä Feature selection: Use correlation insights for predictive modeling",
                "üéØ Segmentation: Create groups based on correlated variables"
            ])

    # Categorical analysis suggestions
    if profile_info['categorical_cols']:
        suggestions.extend([
            "üè∑Ô∏è Cross-tabulation: Analyze relationships between categorical variables",
            "üìä A/B testing: Compare performance across categories",
            "üéØ Customer segmentation: Group analysis by key categorical variables"
        ])

    # Advanced analytics suggestions
    suggestions.extend([
        "ü§ñ Predictive modeling: Build machine learning models for forecasting",
        "üîç Anomaly detection: Identify unusual patterns or outliers",
        "üìà Performance metrics: Define and track KPIs based on findings",
        "üåê External data integration: Enrich with market or economic indicators"
    ])

    for i, suggestion in enumerate(suggestions, 1):
        print(f"  {i}. {suggestion}")

    return suggestions
```

## Main Execution Flow

```python
def analyze_bigquery_data(analysis_request):
    """Main execution function for BigQuery data analysis"""

    print(f"üöÄ Starting Analysis: {analysis_request}")
    print("="*60)

    # Phase 1: Deep thinking
    # [This would be done by the AI agent based on the analytical framework]

    # Phase 2: Data discovery
    datasets = discover_datasets()

    # Extract keywords from analysis request for table discovery
    keywords = analysis_request.lower().split()
    relevant_tables = find_relevant_tables(datasets, keywords)

    if not relevant_tables:
        print("‚ùå No relevant tables found. Exploring all available tables...")
        # Fallback: explore first few tables in each dataset
        for dataset_id in datasets[:3]:
            tables = discover_tables(dataset_id)[:2]
            relevant_tables.extend([(dataset_id, table_id) for table_id in tables])

    # Phase 3: Analysis for each relevant table
    all_results = []

    for dataset_id, table_id in relevant_tables[:3]:  # Limit to 3 tables for efficiency
        try:
            print(f"\n" + "="*60)
            print(f"ANALYZING: {dataset_id}.{table_id}")
            print("="*60)

            # Get sample data for analysis
            df, table_info = explore_table(dataset_id, table_id, sample_size=10000)

            # Profile the data
            profile_info = deep_data_profile(df, f"{dataset_id}.{table_id}")

            # Perform statistical analysis
            stats_results = perform_statistical_analysis(df, profile_info)

            # Generate visualizations
            visualizations = create_comprehensive_visualizations(
                df, profile_info, stats_results, f"{dataset_id}.{table_id}"
            )

            # Generate insights
            insights, recommendations = generate_business_insights(
                df, profile_info, stats_results, analysis_request
            )

            # Generate report
            report = generate_executive_report(
                f"{analysis_request} - {dataset_id}.{table_id}",
                df, profile_info, stats_results, insights, recommendations, visualizations
            )

            # Suggest follow-ups
            followups = suggest_followup_analysis(analysis_request, df, profile_info, stats_results)

            all_results.append({
                'table': f"{dataset_id}.{table_id}",
                'dataframe': df,
                'profile': profile_info,
                'statistics': stats_results,
                'insights': insights,
                'recommendations': recommendations,
                'visualizations': visualizations,
                'report': report,
                'followups': followups
            })

        except Exception as e:
            print(f"‚ùå Error analyzing {dataset_id}.{table_id}: {e}")
            continue

    # Phase 4: Cross-table insights (if multiple tables analyzed)
    if len(all_results) > 1:
        print(f"\nüîó CROSS-TABLE ANALYSIS")
        print("="*60)
        print("Opportunities for joining or comparing tables:")

        # Look for common columns across tables
        common_columns = set(all_results[0]['dataframe'].columns)
        for result in all_results[1:]:
            common_columns &= set(result['dataframe'].columns)

        if common_columns:
            print(f"Common columns found: {list(common_columns)}")
            print("Consider joining tables on these columns for deeper insights.")

    return all_results

# Usage example
if __name__ == "__main__":
    # This would be called with the $ARGUMENTS from the command line
    analysis_request = "$ARGUMENTS"  # Replace with actual input
    results = analyze_bigquery_data(analysis_request)
```

## Quality Assurance & Best Practices

### Error Handling:

- Graceful handling of authentication issues
- Timeout management for large queries
- Memory optimization for large datasets
- Data type conversion error handling

### Performance Optimization:

- Sampling strategies for large tables
- Efficient query patterns
- Visualization memory management
- Progressive disclosure of insights

### Security Considerations:

- Respect data privacy and access controls
- No storage of sensitive data in outputs
- Audit trail of queries executed
- Proper authentication handling

## Output Format

The agent will provide:

1. **Executive Summary** - Key findings and recommendations
2. **Detailed Statistical Report** - Comprehensive analysis results
3. **Visualizations** - Publication-quality charts and graphs
4. **Actionable Insights** - Business intelligence recommendations
5. **Follow-up Suggestions** - Next steps for deeper analysis
6. **Technical Documentation** - Methods and assumptions used

## Usage Examples

```bash
# Analyze customer behavior
python bigquery_agent.py "Analyze customer purchase patterns and identify high-value segments"

# Investigate performance issues
python bigquery_agent.py "Find causes of declining conversion rates in the ecommerce funnel"

# Time series analysis
python bigquery_agent.py "Examine seasonal trends in user engagement metrics"

# Comparative analysis
python bigquery_agent.py "Compare performance metrics across different product categories"
```

This agent combines deep analytical thinking with practical data science techniques to deliver actionable insights from BigQuery data.
